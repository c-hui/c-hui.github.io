---
title: 机器学习基石(1)-感知器
date: 2020-05-01 21:07:17
tags: 机器学习基石
mathjax: true
---

## 问题：信用卡审核
根据用户的申请信息，决定是否通过信用卡申请。
![Credit Approval Problem Revisited](https://pic.downk.cc/item/5eac28aec2a9a83be594bdd5.png)

$\f$是理想的信用卡审核公式， $\mathcal{D}$ 是根据 $\f$ 生成的历史数据，即以前的信用卡申请记录，$\bf{x}$ 表示用户的申请信息，$y$ 表示是否通过审核。

假设集 $\mathcal{H}$ 是候选公式集合，因为我们都不清楚真正的 $f$ 是什么样子的，只能通过学习算法 $\mathcal{A}$ 从 $\mathcal{H}$ 选出最接近 $f$ 的 $g$。最后，我们就可以使用 $g$ 判断是否给新来的申请用户发放信用卡了。

## 假设集合

我们假设用户的每个特征，如年收入等，都对最后的审核结果有一定的影响，或者是正面的，或者是负面的。假设每个特征都有一个权重，我们将权重和特征值相乘的值，表示对审核结果的影响值，我们将所有影响值累加起来，如果大于某个阈值，我们就发放信用卡，如果低于这个阈值，我们就拒绝发放。

设 $\mathbf{x}=\left(x_{1}, x_{2}, \cdots, x_{d}\right)$ 为用户的特征向量，计算用户的加权分数，进行判断：

approve credit if $\quad \sum_{i=1}^{d} w_{i} x_{i}>$ threshold

deny credit if $\sum_{i=1}^{d} w_{i} x_{i}<$ threshold

于是，我们得到 'perceptron' hypothesis:

$$h(\mathbf{x})=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)-\text { threshold }\right)$$

返回值为 $\mathcal{Y}:\{+1(\text { good }),-1(\text { bad })\}$

对 $h(x)$ 向量化：

$$\begin{aligned}
h(\mathbf{x}) &=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)-\text { threshold }\right) \\
&=\operatorname{sign}(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)+\underbrace{(-\text { threshold })}_{w_{0}} \cdot \underbrace{(+1)}_{x_{0}}) \\
&=\operatorname{sign}\left(\sum_{i=0}^{d} w_{i} x_{i}\right) \\
&=\operatorname{sign}\left(\mathbf{w}^{T} \mathbf{x}\right)
\end{aligned}$$

## Perceptron Learning Algorithm

我们需要一个算法，从假设集合 $\mathcal{H}$ 选出一个最接近真实 $f$ 的 $g$, 而 $g$ 由向量 $w$ 表示。

以下是感知器算法：

0. 初始化 $w_0 = 0$, 然后逐步纠正它的错误

for $t=0,1,\ldots$
1. 找到 $w_t$ 错误点：$(x_{n(t)}, y_{n(t)})$，满足

$$\operatorname{sign}\left(\mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}\right) \neq y_{n(t)}$$

2. 尝试纠正错误：

$$\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}$$

直到没有错误产生位置，返回最后的 $\mathbf{w}$ 作为 $f$.

由 PLA 规则：

$$\operatorname{sign}\left(\mathbf{w}_{t}^{T} \mathbf{x}_{n}\right) \neq y_{n}, \quad \mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n} \mathbf{x}_{n}$$

两边同时乘以 $\mathbf{x}_ny_n$, 得到

$$y_{n} \mathbf{w}_{t+1}^{T} \mathbf{x}_{n} \geq y_{n} \mathbf{w}_{t}^{T} \mathbf{x}_{n}$$

由此可知，$\mathbf{w}_{t+1}^{T}\mathbf{x}_n$ 

比 $\mathbf{w}_{t}^{T}\mathbf{x}_n$ 更加接近与 $y_n$ 同符号。

## PLA 证明

$\mathcal{D}$ 线性可分：指数据集存在 $\mathbf{w}$ 对数据集中的点的分类结果都是正确的。

linear separable $\mathcal{D} \Leftrightarrow$ exists perfect $\mathrm{w}_{f}$ such that $y_{n}=\operatorname{sign}\left(\mathrm{w}_{f}^{T} \mathrm{x}_{n}\right)$

1. $\mathrm{w}_f$，对 $\mathrm{x}_{n}$ 都满足：

$$y_{n(t)} \mathbf{w}_{f}^{T} \mathbf{x}_{n(t)} \geq \min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}>0$$

2. 内积 $\mathbf{w}_{f}^{T} \mathbf{w}_{t}$ 满足：

$$\begin{aligned}
\mathbf{w}_{f}^{T} \mathbf{w}_{t+1} &=\mathbf{w}_{f}^{T}\left(\mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}\right) \\
& \geq \mathbf{w}_{f}^{T} \mathbf{w}_{t}+\min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n} \\
&>\mathbf{w}_{f}^{T} \mathbf{w}_{t}+0
\end{aligned}$$

3. $\mathbf{w}_{t}$ 更新当且仅当有错误发生时，则有

$$\operatorname{sign}\left(\mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}\right) \neq y_{n(t)} \Leftrightarrow y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)} \leq 0$$

4. 于是

$$\begin{aligned}
\left\|\mathbf{w}_{t+1}\right\|^{2} &=\left\|\mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \\
&=\left\|\mathbf{w}_{t}\right\|^{2}+2 y_{n(t)} \mathbf{w}_{t}^{T} \mathbf{x}_{n(t)}+\left\|y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \\
& \leq\left\|\mathbf{w}_{t}\right\|^{2}+0+\left\|y_{n(t)} \mathbf{x}_{n(t)}\right\|^{2} \\
& \leq\left\|\mathbf{w}_{t}\right\|^{2}+\max _{n}\left\|y_{n} \mathbf{x}_{n}\right\|^{2}
\end{aligned}$$

5. 从 $\mathbf{w}_{0}=0$ 开始，经过 $T$ 次错误纠正，我们可以得到

$$\begin{aligned}
\frac{\mathbf{w}_{f}^{T}}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{T}}{\left\|\mathbf{w}_{T}\right\|} &>\frac{1}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{f}^{T} \mathbf{w}_{T-1}+\min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}}{\sqrt{\left\|\mathbf{w}_{T-1}\right\|^{2}+\max _{n}\left\|\mathbf{x}_{n}\right\|^{2}}} \\
&>\frac{1}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{f}^{T} \mathbf{w}_{0}+T \min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}}{\sqrt{\left\|\mathbf{w}_{0}\right\|^{2}+T \max _{n}\left\|\mathbf{x}_{n}\right\|^{2}}} \\
&=\frac{1}{\left\|\mathbf{w}_{f}\right\|} \frac{T \min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}}{\sqrt{\operatorname{Tmax}_{n}\left\|\mathbf{x}_{n}\right\|^{2}}} \\
&=\sqrt{T} \frac{\min _{n} y_{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}}{\left\|\mathbf{w}_{f}\right\| \sqrt{\max _{n} \mathbf{w}_{f}^{T} \mathbf{x}_{n}}}
\end{aligned}$$

由此，可知，两个单位向量的内积 $\mathbf{w}_{f}^{T} \mathbf{w}_{t}$ 会越来越大，表明两个向量的夹角越来越小，直到重合。所以，只要数据集是线性可分的，PLA算法总能收敛。

因为向量内积小于等于1，由
$$\frac{\mathbf{w}_{f}^{T}}{\left\|\mathbf{w}_{f}\right\|} \frac{\mathbf{w}_{T}}{\left\|\mathbf{w}_{T}\right\|} \leq 1$$

可知，PLA的经过 $T$ 次就能收敛：
$$T \leq \frac{\max _{n}\left\|\mathbf{x}_{n}\right\|^{2}}{\left(\min _{n} y_{n} \frac{\mathbf{w}_{f}^{T}}{\left\|\mathbf{w}_{f}\right\|} \mathbf{x}_{n}\right)^{2}}$$

## 非线性可分的数据 —— Pocket Algorithm

PLA 算法的有点是足够简单，并且适合所有维度的线性可分的数据集。缺点是，当数据不是线性可分，或者包含错误的数据而不能线性可分时，PLA 算法并不能解决。而且，PLA 的迭代次数我们是不清楚的。

当数据含有少部分的错误数据，导致不能线性可分时，我们只需要得到近似的 $\mathbf{w}_g$，使其在大部分的情况下满足真实的情况：
$$\mathbf{w}_{g} \leftarrow \underset{\mathbf{w}}{\operatorname{argmin}} \sum_{n=1}^{N}\left[\| y_{n} \neq \operatorname{sign}\left(\mathbf{w}^{T} \mathbf{x}_{n}\right)\right]$$

但是，这样的问题的是个NP难题，没有完美的解法。只有近似的解法。

### Pocket Algorithm

Pocket Algorithm 是修改后的PLA算法：

初始化 $\hat{\mathbf{w}}$，For $t = 0,1,\cdots$
1. 随机找到 $\mathbf{w}_t$ 的一个错误点：$\left(\mathbf{x}_{n(t)}, y_{n(t)}\right)$
2. 纠正这个错误：
$$\mathbf{w}_{t+1} \leftarrow \mathbf{w}_{t}+y_{n(t)} \mathbf{x}_{n(t)}$$
3. 如果 $\mathbf{t+1}$ 的错误点比 $\hat{\mathbf{w}}$ 要少，令 $\hat{\mathbf{w}}=\mathbf{t+1}$

迭代足够多的次数，返回 $\hat{\mathbf{w}}$ 作为最后的结果。

--------
附原文笔记：[Perceptron Learning Algorithm (PLA)](http://note.youdao.com/noteshare?id=426dc43c36cced3b98b3cb66d5e0d944)